{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to your project root if needed\n",
    "import os\n",
    "os.chdir(\"c:\\\\Users\\\\Anii\\\\Medical-Chatbot\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load PDF documents\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf_file(data='Data/')\n",
    "print(f\"Loaded {len(extracted_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    chunks = splitter.split_documents(extracted_data)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(f\"Created {len(text_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21767b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Hugging Face embeddings (free & local)\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_hugging_face_embeddings():\n",
    "    return HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "query_embedding = embeddings.embed_query(\"What is the purpose of the study?\")\n",
    "print(f\"Embedding vector length: {len(query_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pinecone\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "index_name = \"medical-chatbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pinecone index (commented out creation and upload, as index exists)\n",
    "# pc = pinecone.Index(index_name)\n",
    "# Uncomment to create or upload if needed\n",
    "# if index_name not in pinecone.list_indexes():\n",
    "#     pinecone.create_index(name=index_name, dimension=384, metric=\"cosine\")\n",
    "# docsearch = PineconeVectorStore.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     index_name=index_name,\n",
    "#     embedding=embeddings\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ab0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Setup Google Gemini (PaLM API) LLM for local testing\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List\n",
    "\n",
    "class GeminiLLM(LLM):\n",
    "    def __init__(self, model: str = \"models/chat-bison-001\", temperature: float = 0.4, max_tokens: int = 512):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gemini\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = genai.chat.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"author\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            max_output_tokens=self.max_tokens,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "llm = GeminiLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de69f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create RAG chain with prompt\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know, don't make up an answer. \"\n",
    "    \"Answer concisely and accurately based on the context provided.\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Test a question\n",
    "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
    "print(\"Answer:\", response.get(\"answer\") or response.get(\"output_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851653f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa119d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
